{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-30T15:31:18.873972Z",
     "iopub.status.busy": "2023-09-30T15:31:18.873202Z",
     "iopub.status.idle": "2023-09-30T15:31:18.87898Z",
     "shell.execute_reply": "2023-09-30T15:31:18.877758Z",
     "shell.execute_reply.started": "2023-09-30T15:31:18.873903Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-09-30T16:21:16.486247Z",
     "iopub.status.busy": "2023-09-30T16:21:16.485508Z",
     "iopub.status.idle": "2023-09-30T16:21:16.510877Z",
     "shell.execute_reply": "2023-09-30T16:21:16.509734Z",
     "shell.execute_reply.started": "2023-09-30T16:21:16.486214Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ghostmaga\\Desktop\\FALL 24\\DeepL\\.venv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "import missingno as msno\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "from tensorflow.keras.preprocessing import text\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Embedding,LSTM,Dropout\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import BertForSequenceClassification, BertTokenizerFast\n",
    "# from transformers import TFDistilBertForSequenceClassification, TFTrainer, TFTrainingArguments\n",
    "from transformers import BertTokenizer#, TFBertForSequenceClassification, BertConfig\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "torch.cuda.is_available()\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-30T15:31:37.093566Z",
     "iopub.status.busy": "2023-09-30T15:31:37.092793Z",
     "iopub.status.idle": "2023-09-30T15:31:37.104701Z",
     "shell.execute_reply": "2023-09-30T15:31:37.10388Z",
     "shell.execute_reply.started": "2023-09-30T15:31:37.093524Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_json_file(filename):\n",
    "    with open(filename) as f:\n",
    "        file = json.load(f)\n",
    "    return file\n",
    "\n",
    "filename = 'intents_exp.json'\n",
    "\n",
    "intents = load_json_file(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-30T15:31:37.107138Z",
     "iopub.status.busy": "2023-09-30T15:31:37.106442Z",
     "iopub.status.idle": "2023-09-30T15:31:37.128029Z",
     "shell.execute_reply": "2023-09-30T15:31:37.126945Z",
     "shell.execute_reply.started": "2023-09-30T15:31:37.107104Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pattern</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Pattern, Tag]\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_df():\n",
    "    df = pd.DataFrame({\n",
    "        'Pattern' : [],\n",
    "        'Tag' : []\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = create_df()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-30T15:31:37.131738Z",
     "iopub.status.busy": "2023-09-30T15:31:37.131508Z",
     "iopub.status.idle": "2023-09-30T15:31:37.560757Z",
     "shell.execute_reply": "2023-09-30T15:31:37.559757Z",
     "shell.execute_reply.started": "2023-09-30T15:31:37.131719Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pattern</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How are you?</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is anyone there?</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hello</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Good day</td>\n",
       "      <td>greeting</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Pattern       Tag\n",
       "0                Hi  greeting\n",
       "1      How are you?  greeting\n",
       "2  Is anyone there?  greeting\n",
       "3             Hello  greeting\n",
       "4          Good day  greeting"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_json_info(json_file, df):\n",
    "    \n",
    "    for intent in json_file['intents']:\n",
    "        \n",
    "        for pattern in intent['patterns']:\n",
    "            \n",
    "            sentence_tag = [pattern, intent['tag']]\n",
    "            df.loc[len(df.index)] = sentence_tag\n",
    "                \n",
    "    return df\n",
    "\n",
    "df = extract_json_info(intents, df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure NLTK resources are downloaded\n",
    "# nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "def extract_json_info(json_file, df):\n",
    "    \n",
    "    for intent in json_file['intents']:\n",
    "        \n",
    "        for pattern in intent['patterns']:\n",
    "            \n",
    "            sentence_tag = [pattern, intent['tag']]\n",
    "            df.loc[len(df.index)] = sentence_tag\n",
    "                \n",
    "    return df\n",
    "\n",
    "df = extract_json_info(intents, df)\n",
    "# df.head()\n",
    "\n",
    "# Function to get synonyms for a word\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name().replace(\"_\", \" \"))\n",
    "    return synonyms\n",
    "\n",
    "# Function to create new patterns with synonyms\n",
    "def expand_patterns_with_synonyms(df):\n",
    "    expanded_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        pattern = row['Pattern']\n",
    "        tag = row['Tag']\n",
    "        words = pattern.split()\n",
    "        new_patterns = [pattern]\n",
    "        \n",
    "        # Generate new patterns by replacing words with synonyms\n",
    "        for word in words:\n",
    "            synonyms = get_synonyms(word)\n",
    "            for synonym in synonyms:\n",
    "                new_patterns.append(pattern.replace(word, synonym))\n",
    "        \n",
    "        # Add all generated patterns with the same tag to the dataframe\n",
    "        for new_pattern in set(new_patterns):\n",
    "            expanded_data.append({'Pattern': new_pattern, 'Tag': tag})\n",
    "    \n",
    "    return pd.DataFrame(expanded_data)\n",
    "\n",
    "df = expand_patterns_with_synonyms(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-30T15:31:37.564532Z",
     "iopub.status.busy": "2023-09-30T15:31:37.563585Z",
     "iopub.status.idle": "2023-09-30T15:31:37.57489Z",
     "shell.execute_reply": "2023-09-30T15:31:37.573852Z",
     "shell.execute_reply.started": "2023-09-30T15:31:37.564481Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df2 = df.copy()\n",
    "# df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-30T15:31:37.578579Z",
     "iopub.status.busy": "2023-09-30T15:31:37.577285Z",
     "iopub.status.idle": "2023-09-30T15:31:37.584656Z",
     "shell.execute_reply": "2023-09-30T15:31:37.583553Z",
     "shell.execute_reply.started": "2023-09-30T15:31:37.578543Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot dataset has 48630 rows and 2 columns\n"
     ]
    }
   ],
   "source": [
    "def print_shape_df(df, ds_name=\"df\"):\n",
    "    print(f\"{ds_name} dataset has {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "    \n",
    "print_shape_df(df, \"Chatbot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-30T15:31:37.586702Z",
     "iopub.status.busy": "2023-09-30T15:31:37.586096Z",
     "iopub.status.idle": "2023-09-30T15:31:37.607304Z",
     "shell.execute_reply": "2023-09-30T15:31:37.606297Z",
     "shell.execute_reply.started": "2023-09-30T15:31:37.586668Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The info of Chatbot dataset\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48630 entries, 0 to 48629\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   Pattern  48630 non-null  object\n",
      " 1   Tag      48630 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 760.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def print_dfInfo(df, ds_name=\"df\"):\n",
    "    print(f\"The info of {ds_name} dataset\\n\")\n",
    "    print(df.info())\n",
    "    \n",
    "print_dfInfo(df, \"Chatbot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-30T15:31:37.609371Z",
     "iopub.status.busy": "2023-09-30T15:31:37.60879Z",
     "iopub.status.idle": "2023-09-30T15:31:37.619815Z",
     "shell.execute_reply": "2023-09-30T15:31:37.618409Z",
     "shell.execute_reply.started": "2023-09-30T15:31:37.609339Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Chatbot dataset has 38 classes\n"
     ]
    }
   ],
   "source": [
    "def num_classes(df, target_col, ds_name=\"df\"):\n",
    "    print(f\"The {ds_name} dataset has {len(df[target_col].unique())} classes\")\n",
    "    \n",
    "num_classes(df, 'Tag', \"Chatbot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-30T16:20:46.733354Z",
     "iopub.status.busy": "2023-09-30T16:20:46.732778Z",
     "iopub.status.idle": "2023-09-30T16:20:46.742777Z",
     "shell.execute_reply": "2023-09-30T16:20:46.741405Z",
     "shell.execute_reply.started": "2023-09-30T16:20:46.733322Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Values in each col in the Chatbot dataset:\n",
      "\n",
      "Pattern    0\n",
      "Tag        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def check_null(df, ds_name='df'):\n",
    "    print(f\"Null Values in each col in the {ds_name} dataset:\\n\")\n",
    "    print(df.isnull().sum())\n",
    "    \n",
    "check_null(df, \"Chatbot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ghostmaga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-30T15:31:38.498179Z",
     "iopub.status.busy": "2023-09-30T15:31:38.49785Z",
     "iopub.status.idle": "2023-09-30T15:31:38.573085Z",
     "shell.execute_reply": "2023-09-30T15:31:38.572072Z",
     "shell.execute_reply.started": "2023-09-30T15:31:38.49815Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "ignore_words=['?', '!', ',', '.']\n",
    "\n",
    "def preprocess_pattern(pattern, stemmer, ignore_words):\n",
    "    words = word_tokenize(pattern.lower())\n",
    "    stemmed_words = [stemmer.stem(word) for word in words if word not in ignore_words]\n",
    "    return \" \".join(stemmed_words)  \n",
    "\n",
    "df['Pattern'] = df['Pattern'].apply(lambda x: preprocess_pattern(x, stemmer, ignore_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-30T15:31:38.589091Z",
     "iopub.status.busy": "2023-09-30T15:31:38.588757Z",
     "iopub.status.idle": "2023-09-30T15:31:38.595942Z",
     "shell.execute_reply": "2023-09-30T15:31:38.594742Z",
     "shell.execute_reply.started": "2023-09-30T15:31:38.589067Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(df['Pattern'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-30T15:31:42.390745Z",
     "iopub.status.busy": "2023-09-30T15:31:42.390155Z",
     "iopub.status.idle": "2023-09-30T15:31:42.4044Z",
     "shell.execute_reply": "2023-09-30T15:31:42.403231Z",
     "shell.execute_reply.started": "2023-09-30T15:31:42.390708Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'hawaii', 'howdi', 'aloha', 'state']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_corpus(series):\n",
    "    words = []\n",
    "    for text in series:\n",
    "        for word in text.split():\n",
    "            words.append(word.strip())\n",
    "    return words\n",
    "\n",
    "corpus = get_corpus(df.Pattern)\n",
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-30T15:31:42.410515Z",
     "iopub.status.busy": "2023-09-30T15:31:42.409395Z",
     "iopub.status.idle": "2023-09-30T15:31:42.41498Z",
     "shell.execute_reply": "2023-09-30T15:31:42.414077Z",
     "shell.execute_reply.started": "2023-09-30T15:31:42.41048Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset contains 263580 words\n"
     ]
    }
   ],
   "source": [
    "print(f\"dataset contains {len(corpus)} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-30T15:31:42.417201Z",
     "iopub.status.busy": "2023-09-30T15:31:42.416556Z",
     "iopub.status.idle": "2023-09-30T15:31:42.43134Z",
     "shell.execute_reply": "2023-09-30T15:31:42.430412Z",
     "shell.execute_reply.started": "2023-09-30T15:31:42.417166Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 13272,\n",
       " 'you': 9518,\n",
       " 'i': 7798,\n",
       " 'what': 6520,\n",
       " 'for': 5572,\n",
       " 'is': 5538,\n",
       " 'can': 5020,\n",
       " 'a': 4834,\n",
       " 'colleg': 4682,\n",
       " 'how': 4670}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counter = Counter(corpus)\n",
    "most_common = counter.most_common(10)\n",
    "most_common = dict(most_common)\n",
    "most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-30T15:31:42.439157Z",
     "iopub.status.busy": "2023-09-30T15:31:42.4365Z",
     "iopub.status.idle": "2023-09-30T15:31:42.449542Z",
     "shell.execute_reply": "2023-09-30T15:31:42.448605Z",
     "shell.execute_reply.started": "2023-09-30T15:31:42.439123Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_top_text_ngrams(corpus, n,g):\n",
    "    vec = CountVectorizer(ngram_range=(1, 1)).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-30T15:31:43.451981Z",
     "iopub.status.busy": "2023-09-30T15:31:43.451625Z",
     "iopub.status.idle": "2023-09-30T15:31:43.460529Z",
     "shell.execute_reply": "2023-09-30T15:31:43.459396Z",
     "shell.execute_reply.started": "2023-09-30T15:31:43.451949Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['greeting',\n",
       " 'goodbye',\n",
       " 'creator',\n",
       " 'name',\n",
       " 'hours',\n",
       " 'number',\n",
       " 'course',\n",
       " 'fees',\n",
       " 'location',\n",
       " 'hostel',\n",
       " 'event',\n",
       " 'document',\n",
       " 'floors',\n",
       " 'syllabus',\n",
       " 'library',\n",
       " 'infrastructure',\n",
       " 'canteen',\n",
       " 'menu',\n",
       " 'placement',\n",
       " 'ithod',\n",
       " 'computerhod',\n",
       " 'extchod',\n",
       " 'principal',\n",
       " 'sem',\n",
       " 'admission',\n",
       " 'scholarship',\n",
       " 'facilities',\n",
       " 'college intake',\n",
       " 'uniform',\n",
       " 'committee',\n",
       " 'random',\n",
       " 'swear',\n",
       " 'vacation',\n",
       " 'sports',\n",
       " 'salutaion',\n",
       " 'task',\n",
       " 'ragging',\n",
       " 'head']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = df2['Tag'].unique().tolist()\n",
    "labels = [s.strip() for s in labels]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-30T15:31:45.631456Z",
     "iopub.status.busy": "2023-09-30T15:31:45.631101Z",
     "iopub.status.idle": "2023-09-30T15:31:45.63873Z",
     "shell.execute_reply": "2023-09-30T15:31:45.635727Z",
     "shell.execute_reply.started": "2023-09-30T15:31:45.631426Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "num_labels = len(labels)\n",
    "id2label = {id:label for id, label in enumerate(labels)}\n",
    "label2id = {label:id for id, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-30T15:31:45.934444Z",
     "iopub.status.busy": "2023-09-30T15:31:45.934113Z",
     "iopub.status.idle": "2023-09-30T15:31:45.941469Z",
     "shell.execute_reply": "2023-09-30T15:31:45.940382Z",
     "shell.execute_reply.started": "2023-09-30T15:31:45.934416Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'greeting',\n",
       " 1: 'goodbye',\n",
       " 2: 'creator',\n",
       " 3: 'name',\n",
       " 4: 'hours',\n",
       " 5: 'number',\n",
       " 6: 'course',\n",
       " 7: 'fees',\n",
       " 8: 'location',\n",
       " 9: 'hostel',\n",
       " 10: 'event',\n",
       " 11: 'document',\n",
       " 12: 'floors',\n",
       " 13: 'syllabus',\n",
       " 14: 'library',\n",
       " 15: 'infrastructure',\n",
       " 16: 'canteen',\n",
       " 17: 'menu',\n",
       " 18: 'placement',\n",
       " 19: 'ithod',\n",
       " 20: 'computerhod',\n",
       " 21: 'extchod',\n",
       " 22: 'principal',\n",
       " 23: 'sem',\n",
       " 24: 'admission',\n",
       " 25: 'scholarship',\n",
       " 26: 'facilities',\n",
       " 27: 'college intake',\n",
       " 28: 'uniform',\n",
       " 29: 'committee',\n",
       " 30: 'random',\n",
       " 31: 'swear',\n",
       " 32: 'vacation',\n",
       " 33: 'sports',\n",
       " 34: 'salutaion',\n",
       " 35: 'task',\n",
       " 36: 'ragging',\n",
       " 37: 'head'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-30T15:31:46.223576Z",
     "iopub.status.busy": "2023-09-30T15:31:46.222889Z",
     "iopub.status.idle": "2023-09-30T15:31:46.229873Z",
     "shell.execute_reply": "2023-09-30T15:31:46.228984Z",
     "shell.execute_reply.started": "2023-09-30T15:31:46.223545Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'greeting': 0,\n",
       " 'goodbye': 1,\n",
       " 'creator': 2,\n",
       " 'name': 3,\n",
       " 'hours': 4,\n",
       " 'number': 5,\n",
       " 'course': 6,\n",
       " 'fees': 7,\n",
       " 'location': 8,\n",
       " 'hostel': 9,\n",
       " 'event': 10,\n",
       " 'document': 11,\n",
       " 'floors': 12,\n",
       " 'syllabus': 13,\n",
       " 'library': 14,\n",
       " 'infrastructure': 15,\n",
       " 'canteen': 16,\n",
       " 'menu': 17,\n",
       " 'placement': 18,\n",
       " 'ithod': 19,\n",
       " 'computerhod': 20,\n",
       " 'extchod': 21,\n",
       " 'principal': 22,\n",
       " 'sem': 23,\n",
       " 'admission': 24,\n",
       " 'scholarship': 25,\n",
       " 'facilities': 26,\n",
       " 'college intake': 27,\n",
       " 'uniform': 28,\n",
       " 'committee': 29,\n",
       " 'random': 30,\n",
       " 'swear': 31,\n",
       " 'vacation': 32,\n",
       " 'sports': 33,\n",
       " 'salutaion': 34,\n",
       " 'task': 35,\n",
       " 'ragging': 36,\n",
       " 'head': 37}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-30T15:31:47.053779Z",
     "iopub.status.busy": "2023-09-30T15:31:47.052759Z",
     "iopub.status.idle": "2023-09-30T15:31:47.066997Z",
     "shell.execute_reply": "2023-09-30T15:31:47.065672Z",
     "shell.execute_reply.started": "2023-09-30T15:31:47.053737Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pattern</th>\n",
       "      <th>Tag</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hello</td>\n",
       "      <td>greeting</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hawaii</td>\n",
       "      <td>greeting</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>howdy</td>\n",
       "      <td>greeting</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aloha State</td>\n",
       "      <td>greeting</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hi</td>\n",
       "      <td>greeting</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pattern       Tag  labels\n",
       "0        hello  greeting       0\n",
       "1       Hawaii  greeting       0\n",
       "2        howdy  greeting       0\n",
       "3  Aloha State  greeting       0\n",
       "4           hi  greeting       0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['labels'] = df2['Tag'].map(lambda x: label2id[x.strip()])\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-30T15:31:49.182567Z",
     "iopub.status.busy": "2023-09-30T15:31:49.182215Z",
     "iopub.status.idle": "2023-09-30T15:31:49.189437Z",
     "shell.execute_reply": "2023-09-30T15:31:49.18834Z",
     "shell.execute_reply.started": "2023-09-30T15:31:49.182537Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'Hawaii', 'howdy', 'Aloha State', 'hi']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = list(df2['Pattern'])\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-30T15:31:49.454118Z",
     "iopub.status.busy": "2023-09-30T15:31:49.453648Z",
     "iopub.status.idle": "2023-09-30T15:31:49.460949Z",
     "shell.execute_reply": "2023-09-30T15:31:49.459953Z",
     "shell.execute_reply.started": "2023-09-30T15:31:49.454091Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = list(df2['labels'])\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-30T15:31:51.051884Z",
     "iopub.status.busy": "2023-09-30T15:31:51.050816Z",
     "iopub.status.idle": "2023-09-30T15:31:51.057964Z",
     "shell.execute_reply": "2023-09-30T15:31:51.056995Z",
     "shell.execute_reply.started": "2023-09-30T15:31:51.051827Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1140/1140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 31ms/step - accuracy: 0.3127 - loss: 2.3111 - val_accuracy: 0.9074 - val_loss: 0.3426\n",
      "Epoch 2/5\n",
      "\u001b[1m1140/1140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 32ms/step - accuracy: 0.9306 - loss: 0.2610 - val_accuracy: 0.9626 - val_loss: 0.1310\n",
      "Epoch 3/5\n",
      "\u001b[1m1140/1140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 30ms/step - accuracy: 0.9717 - loss: 0.1026 - val_accuracy: 0.9821 - val_loss: 0.0684\n",
      "Epoch 4/5\n",
      "\u001b[1m1140/1140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 30ms/step - accuracy: 0.9815 - loss: 0.0657 - val_accuracy: 0.9851 - val_loss: 0.0594\n",
      "Epoch 5/5\n",
      "\u001b[1m1140/1140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 31ms/step - accuracy: 0.9897 - loss: 0.0395 - val_accuracy: 0.9863 - val_loss: 0.0515\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "max_len = max(len(seq) for seq in X_train_seq)\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_len, padding='post')\n",
    "\n",
    "# Build custom model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=128, input_length=max_len),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(64),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(num_labels, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_padded, np.array(y_train),\n",
    "    validation_data=(X_test_padded, np.array(y_test)),\n",
    "    epochs=5,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test_padded).argmax(axis=1)\n",
    "print(\"Classification Report:\")\n",
    "# print(classification_report(y_test, y_pred, target_names=labels))\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save(\"chatbot_model11.h5\")\n",
    "\n",
    "import pickle\n",
    "with open('tokenizer11.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "# Predict function\n",
    "def predict(text, tokenizer, model):\n",
    "    seq = tokenizer.texts_to_sequences([text])\n",
    "    padded = pad_sequences(seq, maxlen=max_len, padding='post')\n",
    "    prediction = model.predict(padded).argmax(axis=1)\n",
    "    return id2label[prediction[0]]\n",
    "\n",
    "# Chat function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Hi! I am your virtual assistant. Type 'quit' to exit.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "Chatbot: I'm sorry, I didn't understand that.\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Chatbot: Good to see you again!\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Chatbot: Goodbye!\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Chatbot: Nazarbayev University offers courses in Engineering, Sciences, Social Sciences, Business, Medicine, and more. For details, visit the official course catalog on the NU website.\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Chatbot: At Nazarbayev University, each school has a dean or program director overseeing the departments. For specific inquiries about the IT program, please contact the School of Engineering and Digital Sciences.\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Chatbot: Each department at Nazarbayev University has its own head. Please specify the department, such as Computer Science or Mechanical Engineering, for more accurate information.\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Chatbot: Nazarbayev University offers courses in Engineering, Sciences, Social Sciences, Business, Medicine, and more. For details, visit the official course catalog on the NU website.\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Chatbot: I'm sorry, I didn't understand that.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def chat(model, tokenizer):\n",
    "     print(\"Chatbot: Hi! I am your virtual assistant. Type 'quit' to exit.\\n\")\n",
    "     text = input(\"User: \").strip().lower()\n",
    "\n",
    "     while text != 'quit':\n",
    "          pred_tag = predict(text, tokenizer, model)\n",
    "          responses = [intent['responses'] for intent in intents['intents'] if intent['tag'] == pred_tag]\n",
    "          response = random.choice(responses[0]) if responses else \"I'm sorry, I didn't understand that.\"\n",
    "          \n",
    "          seq = tokenizer.texts_to_sequences([text])\n",
    "          padded = pad_sequences(seq, maxlen=max_len, padding='post')\n",
    "          \n",
    "          if max(model.predict(padded)[0]) > 0.7:\n",
    "               print(f\"Chatbot: {response}\\n\")\n",
    "          else:\n",
    "               print(\"Chatbot: I'm sorry, I didn't understand that.\\n\")\n",
    "          text = input(\"User: \").strip().lower()\n",
    "\n",
    "# Load and start chatting\n",
    "chat(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 467821,
     "sourceId": 877819,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2915525,
     "sourceId": 5024271,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30527,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
