{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ghostmaga\\Desktop\\FALL 24\\DeepL\\.venv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, GRU, Bidirectional, Dense, Dropout, Conv1D, MaxPooling1D, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification, BertTokenizerFast, BertForSequenceClassification, BertTokenizer, TrainingArguments, Trainer, pipeline\n",
    "import pandas as pd\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import missingno as msno\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 97ms/step - accuracy: 0.0137 - loss: 3.6392 - val_accuracy: 0.0432 - val_loss: 3.6254 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.0481 - loss: 3.6183 - val_accuracy: 0.0719 - val_loss: 3.5971 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.0614 - loss: 3.5864 - val_accuracy: 0.0432 - val_loss: 3.5663 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 0.0561 - loss: 3.5989 - val_accuracy: 0.0432 - val_loss: 3.5401 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.0646 - loss: 3.5213 - val_accuracy: 0.0647 - val_loss: 3.3837 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.0774 - loss: 3.3069 - val_accuracy: 0.1727 - val_loss: 3.1756 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.1164 - loss: 3.1223 - val_accuracy: 0.2446 - val_loss: 3.0553 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.2181 - loss: 2.9172 - val_accuracy: 0.1871 - val_loss: 2.8574 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.2284 - loss: 2.7736 - val_accuracy: 0.3165 - val_loss: 2.6437 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.3082 - loss: 2.5375 - val_accuracy: 0.3741 - val_loss: 2.5506 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.3726 - loss: 2.2943 - val_accuracy: 0.3813 - val_loss: 2.4393 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.3913 - loss: 2.1378 - val_accuracy: 0.4101 - val_loss: 2.3481 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.4139 - loss: 2.0473 - val_accuracy: 0.4676 - val_loss: 2.1691 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.4993 - loss: 1.7254 - val_accuracy: 0.4604 - val_loss: 2.0844 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.4998 - loss: 1.7058 - val_accuracy: 0.6043 - val_loss: 1.9613 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.5820 - loss: 1.3854 - val_accuracy: 0.5468 - val_loss: 2.1293 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.6210 - loss: 1.3067 - val_accuracy: 0.5396 - val_loss: 1.9779 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.6358 - loss: 1.2507 - val_accuracy: 0.5827 - val_loss: 2.0499 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.7026 - loss: 1.0628 - val_accuracy: 0.6187 - val_loss: 1.8582 - learning_rate: 5.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.7205 - loss: 1.0144 - val_accuracy: 0.6187 - val_loss: 1.9036 - learning_rate: 5.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.7596 - loss: 0.9180 - val_accuracy: 0.6187 - val_loss: 1.9142 - learning_rate: 5.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.7371 - loss: 0.8813 - val_accuracy: 0.6475 - val_loss: 1.8983 - learning_rate: 5.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.7645 - loss: 0.8582 - val_accuracy: 0.6331 - val_loss: 1.8768 - learning_rate: 2.5000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.7776 - loss: 0.7646 - val_accuracy: 0.6259 - val_loss: 1.8977 - learning_rate: 2.5000e-04\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6025 - loss: 1.9376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.8581576347351074\n",
      "Test Accuracy: 0.6187050342559814\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 412ms/step\n",
      "Predicted Intent: goodbye\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Load JSON file\n",
    "def load_json_file(filename):\n",
    "    with open(filename) as f:\n",
    "        file = json.load(f)\n",
    "    return file\n",
    "\n",
    "# Create a DataFrame\n",
    "def create_df():\n",
    "    return pd.DataFrame({'Pattern': [], 'Tag': []})\n",
    "\n",
    "# Extract information from JSON and populate the DataFrame\n",
    "def extract_json_info(json_file, df):\n",
    "    for intent in json_file['intents']:\n",
    "        for pattern in intent['patterns']:\n",
    "            df.loc[len(df.index)] = [pattern, intent['tag']]\n",
    "    return df\n",
    "\n",
    "# Preprocess text data\n",
    "def preprocess_pattern(pattern, stemmer, ignore_words):\n",
    "    words = word_tokenize(pattern.lower())\n",
    "    stemmed_words = [stemmer.stem(word) for word in words if word not in ignore_words]\n",
    "    return \" \".join(stemmed_words)\n",
    "\n",
    "# Load intents JSON\n",
    "filename = 'intents_exp.json'\n",
    "intents = load_json_file(filename)\n",
    "\n",
    "# Initialize DataFrame and extract patterns and tags\n",
    "df = create_df()\n",
    "df = extract_json_info(intents, df)\n",
    "\n",
    "# Initialize stemmer and ignore list\n",
    "stemmer = PorterStemmer()\n",
    "ignore_words = stopwords.words('english') + ['?', '!', ',', '.']\n",
    "\n",
    "# Preprocess patterns\n",
    "df['Pattern'] = df['Pattern'].apply(lambda x: preprocess_pattern(x, stemmer, ignore_words))\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Load stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Contraction map\n",
    "CONTRACTIONS = {\n",
    "    \"don't\": \"do not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"isn't\": \"is not\",\n",
    "    # Add more contractions as needed\n",
    "}\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean and normalize text.\n",
    "    - Expands contractions\n",
    "    - Removes special characters, numbers, and punctuations\n",
    "    - Converts to lowercase\n",
    "    \"\"\"\n",
    "    # Expand contractions\n",
    "    for contraction, expansion in CONTRACTIONS.items():\n",
    "        text = re.sub(r'\\b' + contraction + r'\\b', expansion, text)\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower().strip()\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"\n",
    "    Removes stopwords from a list of tokens.\n",
    "    \"\"\"\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    \"\"\"\n",
    "    Lemmatizes tokens to their base form.\n",
    "    \"\"\"\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Full preprocessing pipeline for a single text input.\n",
    "    \"\"\"\n",
    "    # Step 1: Clean text\n",
    "    cleaned_text = clean_text(text)\n",
    "    \n",
    "    # Step 2: Tokenize\n",
    "    tokens = word_tokenize(cleaned_text)\n",
    "    \n",
    "    # Step 3: Remove stopwords\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    \n",
    "    # Step 4: Lemmatize tokens\n",
    "    lemmatized_tokens = lemmatize_tokens(tokens)\n",
    "    \n",
    "    # Step 5: Join tokens back to a single string\n",
    "    return \" \".join(lemmatized_tokens)\n",
    "\n",
    "# Apply the preprocessing pipeline to the DataFrame\n",
    "df['Processed_Pattern'] = df['Pattern'].apply(preprocess_text)\n",
    "\n",
    "# Optional: Extract additional features for analysis (e.g., sentence length)\n",
    "df['Length'] = df['Pattern'].apply(lambda x: len(x.split()))\n",
    "\n",
    "\n",
    "# Prepare labels\n",
    "labels = df['Tag'].unique().tolist()\n",
    "labels = [label.strip() for label in labels]\n",
    "\n",
    "# Map labels to integers\n",
    "label2id = {label: id for id, label in enumerate(labels)}\n",
    "id2label = {id: label for id, label in enumerate(labels)}\n",
    "df['labels'] = df['Tag'].map(lambda x: label2id[x.strip()])\n",
    "\n",
    "# Prepare data for training\n",
    "X = list(df['Processed_Pattern'])\n",
    "y = list(df['labels'])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert text to sequences\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences\n",
    "max_len = 100\n",
    "X_train_pad = pad_sequences(X_train_seq, padding='post', maxlen=max_len)\n",
    "X_test_pad = pad_sequences(X_test_seq, padding='post', maxlen=max_len)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=128, input_length=max_len),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    Dropout(0.3),\n",
    "    Bidirectional(LSTM(64)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(labels), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_pad, np.array(y_train),\n",
    "    validation_data=(X_test_pad, np.array(y_test)),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, lr_scheduler]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_pad, np.array(y_test))\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"intent_classifier_model.h5\")\n",
    "\n",
    "# Example prediction\n",
    "def predict_intent(text, tokenizer, model, label_map):\n",
    "    seq = tokenizer.texts_to_sequences([text])\n",
    "    padded = pad_sequences(seq, padding='post', maxlen=max_len)\n",
    "    pred = model.predict(padded)\n",
    "    return label_map[np.argmax(pred)]\n",
    "\n",
    "example_text = \"How can I reset my password?\"\n",
    "predicted_label = predict_intent(example_text, tokenizer, model, id2label)\n",
    "print(f\"Predicted Intent: {predicted_label}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
