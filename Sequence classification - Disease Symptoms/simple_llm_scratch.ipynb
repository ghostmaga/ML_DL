import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import numpy as np

# Load dataset
data = pd.read_csv("/data_mining_project2/dataset.csv")

# Clean data: Convert symptoms to lowercase and ensure the disease column is clean
data['symptoms'] = data['symptoms'].str.lower()
data['disease'] = data['disease'].str.strip()

# Map the "disease" to a numeric target (since it's a classification problem)
disease_classes = data['disease'].unique()
disease_mapping = {disease: idx for idx, disease in enumerate(disease_classes)}
data['disease_label'] = data['disease'].map(disease_mapping)

# Prepare features (symptoms) and labels (disease)
X = data['symptoms']
y = data['disease_label']

# Split dataset into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 1. CountVectorizer (Bag of Words)
count_vectorizer = CountVectorizer(stop_words='english')
X_train_count = count_vectorizer.fit_transform(X_train)
X_test_count = count_vectorizer.transform(X_test)

# 2. TfidfVectorizer
tfidf_vectorizer = TfidfVectorizer(stop_words='english')
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

# Models to evaluate
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'Support Vector Machine': SVC(kernel='linear', random_state=42)
}

# Function to evaluate models
def evaluate_model(model, X_train, X_test, y_train, y_test, vectorizer_name):
    print(f"Evaluating {model.__class__.__name__} using {vectorizer_name}")
    
    # Train the model
    model.fit(X_train, y_train)
    
    # Predictions
    y_pred = model.predict(X_test)
    
    # Performance metrics
    print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
    print("Confusion Matrix:")
    print(confusion_matrix(y_test, y_pred))
    print("Classification Report:")
    print(classification_report(y_test, y_pred, target_names=disease_classes))
    print("="*60)

# Evaluate models with CountVectorizer features
for model_name, model in models.items():
    evaluate_model(model, X_train_count, X_test_count, y_train, y_test, "CountVectorizer")

# Evaluate models with TfidfVectorizer features
for model_name, model in models.items():
    evaluate_model(model, X_train_tfidf, X_test_tfidf, y_train, y_test, "TfidfVectorizer")

# Optionally, you can compare the accuracy scores directly:
accuracies_count = {}
accuracies_tfidf = {}

for model_name, model in models.items():
    model.fit(X_train_count, y_train)
    y_pred_count = model.predict(X_test_count)
    accuracies_count[model_name] = accuracy_score(y_test, y_pred_count)
    
    model.fit(X_train_tfidf, y_train)
    y_pred_tfidf = model.predict(X_test_tfidf)
    accuracies_tfidf[model_name] = accuracy_score(y_test, y_pred_tfidf)

# Print out comparison of accuracies
print("\nAccuracy Comparison (CountVectorizer vs. TfidfVectorizer):")
for model_name in models.keys():
    print(f"{model_name}: CountVectorizer Accuracy = {accuracies_count[model_name]:.4f}, TfidfVectorizer Accuracy = {accuracies_tfidf[model_name]:.4f}")
