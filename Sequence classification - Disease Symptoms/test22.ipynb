{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\ghostmaga\\desktop\\fall 24\\deepl\\.venv\\lib\\site-packages (0.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        disease                                           symptoms  \\\n",
      "0           flu  fever,cough,sore throat,runny or stuffy nose,m...   \n",
      "1    bronchitis  cough,mucus production,shortness of breath,che...   \n",
      "2     pneumonia  fever,cough,shortness of breath,chest pain,fat...   \n",
      "3  heart attack  chest pain,shortness of breath,nausea,vomiting...   \n",
      "4        stroke  sudden weakness,numbness on one side of the bo...   \n",
      "\n",
      "                                               cures  \\\n",
      "0           over-the-counter medications,rest,fluids   \n",
      "1  antibiotics,over-the-counter medications,rest,...   \n",
      "2  antibiotics,over-the-counter medications,rest,...   \n",
      "3                         emergency medical services   \n",
      "4                         emergency medical services   \n",
      "\n",
      "                        doctor     risk level  \n",
      "0    family doctor,urgent care      low (0.1%  \n",
      "1  family doctor,pulmonologist      low (0.5%  \n",
      "2  family doctor,pulmonologist  moderate (1%)  \n",
      "3                 cardiologist     high (20%)  \n",
      "4                 neurologist      high (15%)  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 99/99 [00:00<00:00, 3571.98 examples/s]\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m tokenized_gpt2_datasets \u001b[38;5;241m=\u001b[39m tokenized_gpt2_datasets\u001b[38;5;241m.\u001b[39mremove_columns([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msymptoms\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Tokenizer and preprocess for T5\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m t5_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mT5Tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt5-small\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mt5_tokenize_function\u001b[39m(examples):\n\u001b[0;32m     31\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSymptoms: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msymptoms\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Disease:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m examples]\n",
      "File \u001b[1;32mc:\\Users\\ghostmaga\\Desktop\\FALL 24\\DeepL\\.venv\\lib\\site-packages\\transformers\\utils\\import_utils.py:1651\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1650\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 1651\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ghostmaga\\Desktop\\FALL 24\\DeepL\\.venv\\lib\\site-packages\\transformers\\utils\\import_utils.py:1639\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1637\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[0;32m   1638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m-> 1639\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[1;31mImportError\u001b[0m: \nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "# Step 1: Load Data\n",
    "df = pd.read_csv('dataset.csv')  # Make sure the CSV file is in the correct directory\n",
    "print(df.head())\n",
    "\n",
    "# Step 2: Preprocess Data\n",
    "# Convert the DataFrame into a Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Tokenizer and preprocess for GPT-2\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token  # GPT-2 requires a padding token\n",
    "\n",
    "def gpt2_tokenize_function(examples):\n",
    "    inputs = examples['symptoms']\n",
    "    model_inputs = gpt2_tokenizer(inputs, truncation=True, padding=\"max_length\", max_length=128)\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize the dataset for GPT-2\n",
    "tokenized_gpt2_datasets = dataset.map(gpt2_tokenize_function, batched=True)\n",
    "tokenized_gpt2_datasets = tokenized_gpt2_datasets.remove_columns([\"symptoms\"])\n",
    "\n",
    "# Tokenizer and preprocess for T5\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "def t5_tokenize_function(examples):\n",
    "    inputs = [f\"Symptoms: {item['symptoms']} Disease:\" for item in examples]\n",
    "    targets = examples['disease']\n",
    "    model_inputs = t5_tokenizer(inputs, padding=\"max_length\", truncation=True, max_length=128)\n",
    "    labels = t5_tokenizer(targets, padding=\"max_length\", truncation=True, max_length=64)\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize the dataset for T5\n",
    "tokenized_t5_datasets = dataset.map(t5_tokenize_function, batched=True)\n",
    "tokenized_t5_datasets = tokenized_t5_datasets.remove_columns([\"symptoms\", \"disease\"])\n",
    "\n",
    "# Step 3: Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./model\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_steps=500,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_steps=100,\n",
    "    learning_rate=5e-5,\n",
    ")\n",
    "\n",
    "# Step 4: Initialize Models and Trainers\n",
    "\n",
    "# GPT-2 Model for Disease Prediction\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "gpt2_trainer = Trainer(\n",
    "    model=gpt2_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_gpt2_datasets,\n",
    "    eval_dataset=tokenized_gpt2_datasets,\n",
    ")\n",
    "\n",
    "# T5 Model for Disease Prediction\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "t5_trainer = Trainer(\n",
    "    model=t5_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_t5_datasets,\n",
    "    eval_dataset=tokenized_t5_datasets,\n",
    ")\n",
    "\n",
    "# Step 5: Train Both Models\n",
    "\n",
    "# Train GPT-2\n",
    "print(\"Training GPT-2 model...\")\n",
    "gpt2_trainer.train()\n",
    "\n",
    "# Train T5\n",
    "print(\"Training T5 model...\")\n",
    "t5_trainer.train()\n",
    "\n",
    "# Save the models\n",
    "gpt2_model.save_pretrained(\"./gpt2_disease_model\")\n",
    "gpt2_tokenizer.save_pretrained(\"./gpt2_disease_model\")\n",
    "\n",
    "t5_model.save_pretrained(\"./t5_disease_model\")\n",
    "t5_tokenizer.save_pretrained(\"./t5_disease_model\")\n",
    "\n",
    "# Step 6: Evaluate the Models (on validation data)\n",
    "\n",
    "# Example of how to evaluate the model after training\n",
    "def evaluate_model(model, tokenizer, dataset):\n",
    "    # Get predictions\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    for example in dataset:\n",
    "        inputs = example['symptoms']\n",
    "        inputs_tokenized = tokenizer(inputs, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
    "        \n",
    "        # GPT-2 does not return label directly, we need to generate the output\n",
    "        output = model.generate(inputs_tokenized['input_ids'])\n",
    "        predicted_disease = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Append predicted and actual values\n",
    "        predictions.append(predicted_disease)\n",
    "        labels.append(example['disease'])\n",
    "    \n",
    "    # Calculate accuracy and F1 Score\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='micro')\n",
    "    conf_matrix = confusion_matrix(labels, predictions)\n",
    "    \n",
    "    return accuracy, f1, conf_matrix\n",
    "\n",
    "# Evaluate GPT-2\n",
    "print(\"Evaluating GPT-2...\")\n",
    "gpt2_accuracy, gpt2_f1, gpt2_conf_matrix = evaluate_model(gpt2_model, gpt2_tokenizer, dataset)\n",
    "\n",
    "print(\"GPT-2 Accuracy:\", gpt2_accuracy)\n",
    "print(\"GPT-2 F1 Score:\", gpt2_f1)\n",
    "print(\"GPT-2 Confusion Matrix:\\n\", gpt2_conf_matrix)\n",
    "\n",
    "# Evaluate T5\n",
    "print(\"Evaluating T5...\")\n",
    "t5_accuracy, t5_f1, t5_conf_matrix = evaluate_model(t5_model, t5_tokenizer, dataset)\n",
    "\n",
    "print(\"T5 Accuracy:\", t5_accuracy)\n",
    "print(\"T5 F1 Score:\", t5_f1)\n",
    "print(\"T5 Confusion Matrix:\\n\", t5_conf_matrix)\n",
    "\n",
    "# Step 7: Compare the Models\n",
    "if gpt2_f1 > t5_f1:\n",
    "    print(\"GPT-2 performs better.\")\n",
    "else:\n",
    "    print(\"T5 performs better.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
